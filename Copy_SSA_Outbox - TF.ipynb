{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ca7f270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from numpy.random import rand\n",
    "from IPython.display import display\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import chi2, mutual_info_classif, f_classif, SelectKBest\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.utils.validation import column_or_1d\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "random_state = 42\n",
    "import warnings\n",
    "\n",
    "# Suppress FutureWarning\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6937325b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normally we have to import the dataset\n",
    "data_url = 'Datasets/Ovarian_cancer.csv'\n",
    "data = pd.read_csv(data_url)\n",
    "#data.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d00d8633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(253, 15155)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape #just to check the number of instances and features/variables our dataset have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "937f2f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.isnull().sum() #We check to see if our datasets has any missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5517d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have two classes:  ['Normal' 'Cancer']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Class\n",
       "Cancer    162\n",
       "Normal     91\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets check the counts of the class types in our datasets\n",
    "print(\"We have two classes: \", data['Class'].unique())\n",
    "data.value_counts('Class')\n",
    "# This tells us we have 162 instances of type cancer and 91 of type nirmal. Thus we need to do data imbalance handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2759812b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#machine learning algorithms cannot work with categoricla values, thus he has to label out target to numeric values\n",
    "label_encoder = LabelEncoder()\n",
    "data['Class'] = label_encoder.fit_transform(data['Class'])\n",
    "data.Class.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "138e35fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    162\n",
       "1     91\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Class.value_counts() #We see that the cancer class [0] is the majority class with 162 counts of samoples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01bb6c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data.copy()#we make a copy of our data for future references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "307e1885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confirming to ensure its 162 samples in the majority class cancer:  162\n",
      "confirming to ensure its 91 samples in the minority class cancer:  91\n"
     ]
    }
   ],
   "source": [
    "#Now let us handle the imbalancenature of our dataset\n",
    "#we first haave to separate the two classes\n",
    "majority_class_cancer = data[data['Class'] == 0]\n",
    "minority_class_normal = data[data['Class'] == 1]\n",
    "\n",
    "print(\"confirming to ensure its 162 samples in the majority class cancer: \",len(majority_class_cancer.Class))\n",
    "print(\"confirming to ensure its 91 samples in the minority class cancer: \",len(minority_class_normal.Class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04101637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the shape of our data once more: \n",
      " 0    162\n",
      "1    162\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Upsample minority class to equal majority class\n",
    "minority_upsample = resample(minority_class_normal, replace = True, n_samples = len(majority_class_cancer), random_state = random_state)\n",
    "# Combine majority class with upsampled minority class\n",
    "data_combined = pd.concat([majority_class_cancer, minority_upsample])\n",
    "print(\"Checking the shape of our data once more: \\n\", data_combined.Class.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab29ab9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not all elements have the same min value.\n",
      "All elements have the same max value.\n"
     ]
    }
   ],
   "source": [
    "# Display summary statistics to check the range of values so as to know if you need scaling\n",
    "#summary_stats = data.describe()\n",
    "#print(summary_stats)\n",
    "\n",
    "# Check the minimum and maximum values for each feature\n",
    "min_values = data.min()\n",
    "if min_values.nunique() == 0.0:\n",
    "    print(\"All elements have the same min value.\")\n",
    "else:\n",
    "    print(\"Not all elements have the same min value.\")\n",
    "\n",
    "max_values = data.max()\n",
    "if max_values.nunique() == 1.0:\n",
    "    print(\"All elements have the same max value.\")\n",
    "else:\n",
    "    print(\"Not all elements have the same max value.\")\n",
    "#from the output we need to perform a minmax scaling to make sure all our values are withing -1 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57e2b936",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we split our dataframe into features and target\n",
    "features = data.drop(['Class'], axis = 1) #dropping the target column\n",
    "target = data.loc[:,['Class']] #picking only the target column\n",
    "#features.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44ec0c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = MinMaxScaler(feature_range = (0,1))\n",
    "data_scaled = min_max_scaler.fit_transform(features)\n",
    "data_scaled = pd.DataFrame(data_scaled, columns = features.columns)\n",
    "#data_scaled.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5037a874",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SELECTION WITH CHI-SQUARE TEST\n",
    "data_chi = SelectKBest(score_func = chi2)\n",
    "data_chi.fit(data_scaled, target)\n",
    "\n",
    "Table_score = pd.DataFrame(data_chi.scores_)\n",
    "Table_names = pd.DataFrame(data2.columns)\n",
    "chi_table = pd.concat([Table_names, Table_score], axis = 1)\n",
    "chi_table.columns = ['Feature_Names', 'Scores']\n",
    "chi_table.sort_values(by = 'Scores', ascending = False, inplace = True)\n",
    "#display(chi_table)\n",
    "#pd.set_option('display.max_rows', None)\n",
    "#pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b964e477",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SELECTION WITH MUTUAL INFORMATION\n",
    "data_mi = SelectKBest(score_func = mutual_info_classif)\n",
    "target_mi = target.copy()\n",
    "#target_mi = target.ravel()\n",
    "\n",
    "target_mi = column_or_1d(target_mi)\n",
    "\n",
    "data_mi.fit(data_scaled, target_mi)\n",
    "\n",
    "Table_score = pd.DataFrame(data_mi.scores_)\n",
    "Table_names = pd.DataFrame(data2.columns)\n",
    "mi_table = pd.concat([Table_names, Table_score], axis = 1)\n",
    "mi_table.columns = ['Feature_Names', 'Scores']\n",
    "mi_table.sort_values(by = 'Scores', ascending = False, inplace = True)\n",
    "#display(mi_table)\n",
    "#pd.set_option('display.max_rows', None)\n",
    "#pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f999006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Now from our methodology, we aim to use 100, 200, 300, 400, and 500 features where the KNN will\n",
    "automnatically select the feature subset with better accuracy before paissing to the SSA'''\n",
    "#we will roughly select 1000 top features from each of the filter methods\n",
    "Top_1000_chi = chi_table.head(1000)\n",
    "Top_1000_mi = mi_table.head(1000)\n",
    "\n",
    "#Now we merge the two top 1000 features from each filter method to remove duplicates. The scoring is again done automatically.\n",
    "merged_table = pd.concat([Top_1000_chi, Top_1000_mi], axis = 0)\n",
    "merged_table.shape\n",
    "#display(merged_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43ae217a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we again ensure the features are ranked based on scores\n",
    "merged_table.sort_values(by = 'Scores', ascending = False, inplace = True)\n",
    "#display(merged_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2e9f6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any rows have the same value in the specified column\n",
    "duplicate_values = merged_table.duplicated(subset=['Feature_Names'], keep=False)\n",
    "# Filter the DataFrame to show only the rows with duplicate values\n",
    "rows_with_duplicates = merged_table[duplicate_values]\n",
    "#print(rows_with_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db5d6a2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1293, 2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We realised that there are duplicates of features in the dataframe, thus we wil drop the one with a lower rank\n",
    "# Drop duplicates based on the specified column, keeping the first occurrence\n",
    "drop_duplicate = merged_table.drop_duplicates(subset = ['Feature_Names'], keep = 'first')\n",
    "'''This will drop one of the rows that came lower in the DataFrame based on the specified\n",
    "column name and keep only the first occurrence of the duplicate values''';\n",
    "drop_duplicate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b3bc33b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we select top 100 \n",
    "Top_100 = drop_duplicate.head(100)\n",
    "Top_100_features = Top_100['Feature_Names'].tolist()\n",
    "Top_100_data = data2.loc[:,Top_100_features]\n",
    "\n",
    "#Now we select top 200 \n",
    "Top_200 = drop_duplicate.head(200)\n",
    "Top_200_features = Top_200['Feature_Names'].tolist()\n",
    "Top_200_data = data2.loc[:,Top_200_features]\n",
    "\n",
    "#Now we select top 300 \n",
    "Top_300 = drop_duplicate.head(300)\n",
    "Top_300_features = Top_300['Feature_Names'].tolist()\n",
    "Top_300_data = data2.loc[:,Top_300_features]\n",
    "\n",
    "#Now we select top 400 \n",
    "Top_400 = drop_duplicate.head(400)\n",
    "Top_400_features = Top_400['Feature_Names'].tolist()\n",
    "Top_400_data = data2.loc[:,Top_400_features]\n",
    "\n",
    "#Now we select top 500 \n",
    "Top_500 = drop_duplicate.head(500)\n",
    "Top_500_features = Top_500['Feature_Names'].tolist()\n",
    "Top_500_data = data2.loc[:,Top_500_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e9107687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Top_100_data.shape[1] #sample to look at how it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "481a32ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.ravel(target) #converting our target variable into a numpy array\n",
    "#target = target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "056f1f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rates of the 5 splits [0.04, 0.04, 0.0, 0.0, 0.04]\n",
      "Index of best split:  2\n",
      "Shape of selected split:  (253, 300)\n",
      "Training set:(228, 300)\n",
      "Testing set:(25, 300)\n",
      "Training_Label set:(228,)\n",
      "Testing_Lable set:(25,)\n"
     ]
    }
   ],
   "source": [
    "data_sizes = [Top_100_data.values, Top_200_data.values, Top_300_data.values,Top_400_data.values,Top_500_data.values]\n",
    "best_size = []\n",
    "\n",
    "for data_size in data_sizes:\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    for train_index, test_index in kf.split(data_size):\n",
    "        X_train, X_test = data_size[train_index], data_size[test_index]\n",
    "        y_train, y_test = target[train_index], target[test_index]\n",
    "\n",
    "        selection_model = KNeighborsClassifier(n_neighbors = 5)\n",
    "        selection_model.fit(X_train, y_train)\n",
    "        y_pred = selection_model.predict(X_test)\n",
    "        error_rate = 1 - (accuracy_score(y_test, y_pred))\n",
    "        \n",
    "    best_size.append(round(error_rate,2))\n",
    "    #print(f'The error rate of {data_size.shape[1]} top features is: {round(error_rate,2)}, while the accuracy id {1- (round(error_rate,2))}%')\n",
    "    #print('\\n')\n",
    "print(\"Error rates of the 5 splits\", best_size)\n",
    "\n",
    "#we auotmatically select the best subset size based on the error rate value and use it in our SSA\n",
    "error_index = best_size.index(min(best_size)) #find the min value in best_size ie.,min error rate = best acccuracy\n",
    "selected_Top_data = data_sizes[error_index] # Access the corresponding element in data_sizes using the index of min value of best_size\n",
    "\n",
    "print('Index of best split: ', error_index) #printing for debugging\n",
    "print('Shape of selected split: ', selected_Top_data.shape) #printing for debugging\n",
    "\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=random_state)\n",
    "#Because we are using the same value of k and random state, we will always get same train-test split and hence, same result\n",
    "\n",
    "for train_index, test_index in kf.split(selected_Top_data):\n",
    "    x_train, x_test = selected_Top_data[train_index], selected_Top_data[test_index]\n",
    "    y_train, y_test = target[train_index], target[test_index]\n",
    "\n",
    "fold = {'xt': x_train, 'yt': y_train, 'xv': x_test, 'yv': y_test} #Requirement for our SSA algorithm\n",
    "#We print out to see the number of instances in each of the classes\n",
    "print(f'Training set:{x_train.shape}')\n",
    "print(f'Testing set:{x_test.shape}')\n",
    "print(f'Training_Label set:{y_train.shape}')\n",
    "print(f'Testing_Lable set:{y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2516339d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ff6aa245",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We start defining our SSA funtions from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0a040290",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random population initailization\n",
    "def init_position(lb, ub, N, dim):\n",
    "    X = np.zeros([N, dim], dtype='float')\n",
    "    for i in range(N):\n",
    "        for d in range(dim):\n",
    "            X[i,d] = lb[0,d] + (ub[0,d] - lb[0,d]) * np.random.rand()\n",
    "            #X[i,d] = lb[d] + (ub[d] - lb[d]) * np.random.rand()\n",
    "            \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c9a39e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def binary_conversion(X, thres, N, dim):\n",
    "\n",
    "    Xbin = np.zeros([N, dim], dtype='int')\n",
    "    for i in range(N):\n",
    "        for d in range(dim):\n",
    "            if sigmoid(X[i, d]) > thres:\n",
    "                Xbin[i, d] = 1\n",
    "            else:\n",
    "                Xbin[i, d] = 0\n",
    "    return Xbin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ccae4a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making our search space to remian between 0 and 1\n",
    "def boundary(x, lb, ub):\n",
    "    if x < lb:\n",
    "        x = lb\n",
    "    if x > ub:\n",
    "        x = ub\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0645b918",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Error rate/accuracy\n",
    "def error_rate(xtrain, ytrain, x, opts):\n",
    "    #parameters\n",
    "    k = opts['k']\n",
    "    \n",
    "    fold = opts['fold']\n",
    "    xt = fold['xt']\n",
    "    yt = fold['yt']\n",
    "    xv = fold['xv']\n",
    "    yv = fold['yv']\n",
    "    \n",
    "    #Number of instances\n",
    "    num_train = np.size(xt, 0)\n",
    "    num_valid = np.size(xv, 0)\n",
    "    #Define selected features\n",
    "    xtrain = xt[:, x == 1]\n",
    "    ytrain = yt.reshape(num_train)\n",
    "    xvalid = xv[:, x == 1]\n",
    "    yvalid = yv.reshape(num_valid)\n",
    "    \n",
    "    #training our KNN model\n",
    "    mdl = KNeighborsClassifier(n_neighbors=k)\n",
    "    mdl.fit(xtrain, ytrain)\n",
    "    \n",
    "    #making predictions\n",
    "    ypred = mdl.predict(xvalid)\n",
    "    acc = accuracy_score(ypred, yvalid)\n",
    "    #acc = np.sum(yvalid == ypred) / num_valid\n",
    "    error = 1 - acc\n",
    "    \n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a369815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Error rate and Feature Size (Fitness Fucntion)\n",
    "def Fun(xtrain, ytrain, x, opts):\n",
    "    #parameters\n",
    "    alpha = 0.99 #default value in literature\n",
    "    beta = 1 - alpha #default value in literature\n",
    "    \n",
    "    #original feature size\n",
    "    max_feat = len(x)\n",
    "    #number of selected features\n",
    "    num_feat = np.sum(x==1)\n",
    "    #If no feature is selected smiles (impossible)\n",
    "    if num_feat == 0:\n",
    "        cost = 1 #Add your code if you like\n",
    "    else:\n",
    "        #get error rate fucntion\n",
    "        error = error_rate(xtrain, ytrain, x, opts)\n",
    "        #objective/fitness fucntion\n",
    "        cost = alpha * error + beta * (num_feat/max_feat)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ca5986e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_fs(xtrain, ytrain, opts):\n",
    "    #our parameters\n",
    "    ub = 1\n",
    "    lb = 0\n",
    "    thres = .5\n",
    "    \n",
    "    N = opts['N']\n",
    "    max_iter = opts['T']\n",
    "    \n",
    "    #The dimension of our dataset\n",
    "    dim = np.size(xtrain, 1)\n",
    "    if np.size(lb) == 1:\n",
    "        ub = ub * np.ones([1, dim], dtype = 'float')\n",
    "        lb = lb * np.ones([1, dim], dtype = 'float')\n",
    "        \n",
    "    #Initialize position of salps\n",
    "    X = init_position(lb, ub, N, dim)\n",
    "    \n",
    "    fit = np.zeros([N, 1], dtype = 'float')\n",
    "    Xf = np.zeros([1, dim], dtype = 'float')\n",
    "    fitF = float('inf')\n",
    "    curve = np.zeros([1, max_iter], dtype = 'float')\n",
    "    t = 0\n",
    "    \n",
    "    while t < max_iter:\n",
    "        #binary conversion\n",
    "        Xbin = binary_conversion(X, thres, N, dim)\n",
    "        \n",
    "        #Fitness\n",
    "        for i in range(N):\n",
    "            fit[i,0] = Fun(xtrain, ytrain, Xbin[i,:], opts)\n",
    "            if fit[i,0] < fitF:\n",
    "                Xf[0,:] = X[i,:]\n",
    "                fitF = fit[i,0]\n",
    "                \n",
    "        #store results\n",
    "        curve[0,t] = fitF.copy()\n",
    "        print('Iteration: ', t + 1)\n",
    "        print(\"Best (SSA): \", curve[0,t])\n",
    "        t = t + 1\n",
    "        \n",
    "        #Compute coefficent, c1 from SSA algorithm\n",
    "        c1 = 2 * np.exp(-(4 * t / max_iter)**2)\n",
    "        \n",
    "        for i in range(N):\n",
    "            #First leader update\n",
    "            if i == 0:\n",
    "                for d in range(dim):\n",
    "                    #coefficient of c2 and c3 in the range of 0 - 1\n",
    "                    c2 = np.random.rand()\n",
    "                    c3 = np.random.rand()\n",
    "                    #leader update\n",
    "                    if c3>=.5:\n",
    "                        X[i,d] = Xf[0,d] + c1 * ((ub[0,d] -lb[0,d]) * c2 + lb[0,d])\n",
    "                    else:\n",
    "                        X[i,d] = Xf[0,d] - c1 * ((ub[0,d] - lb[0,d]) * c2 + lb[0,d])\n",
    "                        \n",
    "                    #boundary\n",
    "                    X[i,d] = boundary(X[i,d], lb[0,d], ub[0,d])\n",
    "                \n",
    "            #salp update\n",
    "            elif i >= 1:\n",
    "                for d in range(dim):\n",
    "                    #salp update by following leader salp\n",
    "                    X[i,d] =(X[i,d] + X[i - 1, d]) / 2\n",
    "                    #boundary\n",
    "                    X[i,d] = boundary(X[i,d], lb[0,d], ub[0,d])\n",
    "                \n",
    "    \n",
    "    #Best feature subset\n",
    "    Gbin = binary_conversion(Xf, thres, 1, dim)\n",
    "    Gbin = Gbin.reshape(dim)\n",
    "    pos  = np.asarray(range(0,dim))\n",
    "    sel_index = pos[Gbin == 1]\n",
    "    num_feat = len(sel_index)\n",
    "    \n",
    "    #create dictionary\n",
    "    ssa_data = {'sf': sel_index, 'c': curve, 'nf': num_feat}\n",
    "    \n",
    "    return ssa_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bd0e9f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now you run our SSA algorithm\n",
    "#Now we set the general parameters\n",
    "K = 5 #K value in KNN\n",
    "N = 30 #Population size/swarm size\n",
    "T = 60 #Max iteration/generations\n",
    "opts = {'k': K, 'fold': fold, 'N': N, 'T': T}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eac9c0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  1\n",
      "Best (SSA):  0.010000000000000009\n",
      "Iteration:  2\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  3\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  4\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  5\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  6\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  7\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  8\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  9\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  10\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  11\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  12\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  13\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  14\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  15\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  16\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  17\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  18\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  19\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  20\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  21\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  22\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  23\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  24\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  25\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  26\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  27\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  28\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  29\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  30\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  31\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  32\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  33\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  34\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  35\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  36\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  37\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  38\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  39\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  40\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  41\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  42\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  43\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  44\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  45\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  46\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  47\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  48\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  49\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  50\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  51\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  52\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  53\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  54\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  55\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  56\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  57\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  58\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  59\n",
      "Best (SSA):  0.006100000000000006\n",
      "Iteration:  60\n",
      "Best (SSA):  0.006100000000000006\n"
     ]
    }
   ],
   "source": [
    "#Now we perform our feature selection by calling the main module\n",
    "feature_selection_mdl = main_fs(selected_Top_data, target, opts) #feature selection model\n",
    "sf = feature_selection_mdl['sf'] #index of selected features\n",
    "#print(sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b049db64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model with selected features\n",
    "num_train = np.size(x_train,0)#checks the number of samples slected in the x_train and assign to num_train\n",
    "num_test = np.size(x_test, 0)#checks the number of samples slected in the x_train and assign to num_train\n",
    "xtrain = x_train[:,sf]\n",
    "ytrain = y_train.reshape(num_train)\n",
    "xtest = x_test[:,sf]\n",
    "ytest = y_test.reshape(num_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ce155687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we build our model i.e., the classifier\n",
    "KNN_model = KNeighborsClassifier(n_neighbors = 5)\n",
    "KNN_model.fit(xtrain, ytrain)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9a35213c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method to calculate accuracy for our unseen data\n",
    "def train_test_accuracy(model, xtest, ytest, name = ''):\n",
    "    y_pred = model.predict(xtest)\n",
    "    test_accuracy = round(accuracy_score(y_pred, ytest)*100, 2)\n",
    "    test_F1_measure = round(f1_score(y_pred, ytest)*100,2)\n",
    "    error = 1 - round(accuracy_score(y_pred, ytest), 3)\n",
    "        \n",
    "    return print(\"The ACCURACY SCORE is: {} \\nThe F1_MEASURE is {} \\nThe error rate is: {}\". format(test_accuracy, test_F1_measure, error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e00a73d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ACCURACY SCORE is: 100.0 \n",
      "The F1_MEASURE is 100.0 \n",
      "The error rate is: 0.0\n"
     ]
    }
   ],
   "source": [
    "#Calling our fucntion to display result\n",
    "train_test_accuracy(KNN_model, xtest, ytest, name = 'SSA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a515f1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of FEATURES selected is: 183\n"
     ]
    }
   ],
   "source": [
    "#Now we check the number of selected features\n",
    "Number_of_features = len(sf)\n",
    "print(f'The number of FEATURES selected is: {Number_of_features}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a539d17f",
   "metadata": {},
   "source": [
    "#### End of project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc15247",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bac381",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
